{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f6395c",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; overflow: hidden;\">\n",
    "    <div style=\"width: 150px; float: left;\"> <img src=\"https://raw.githubusercontent.com/DataForScience/Networks/master/data/D4Sci_logo_ball.png\" alt=\"Data For Science, Inc\" align=\"left\" border=\"0\" width=150px> </div>\n",
    "    <div style=\"float: left; margin-left: 10px;\"> <h1>Generative AI with OpenAI API</h1>\n",
    "<h1>GPT Models</h1>\n",
    "        <p>Bruno Gon√ßalves<br/>\n",
    "        <a href=\"http://www.data4sci.com/\">www.data4sci.com</a><br/>\n",
    "            @bgoncalves, @data4sci</p></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f57712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import termcolor\n",
    "from termcolor import colored\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "import tqdm as tq\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b834f9a",
   "metadata": {},
   "source": [
    "We start by printing out the versions of the libraries we're using for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876a1b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.11.7\n",
      "IPython version      : 8.12.3\n",
      "\n",
      "Compiler    : Clang 14.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 23.4.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 16\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 944fe12b93e5c9f60a6dc6b0d18688e891a1249f\n",
      "\n",
      "pandas    : 2.1.4\n",
      "json      : 2.0.9\n",
      "termcolor : 2.4.0\n",
      "matplotlib: 3.8.0\n",
      "tqdm      : 4.66.2\n",
      "numpy     : 1.26.4\n",
      "watermark : 2.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfbc9e5",
   "metadata": {},
   "source": [
    "Load default figure style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7f3a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('d4sci.mplstyle')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32f69a",
   "metadata": {},
   "source": [
    "# Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9d274",
   "metadata": {},
   "source": [
    "The first step is generate API key on the OpenAI website and store it as the \"OPENAI_API_KEY\" variable in your local environment. Without it we won't be able to do anything. You can find your API key in your using settings: https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1b830-97e3-443a-811c-ddd20d2ecbb9",
   "metadata": {},
   "source": [
    "Then we are ready to instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5f43171-b255-4219-a5dc-cc4535a47e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60dc01d",
   "metadata": {},
   "source": [
    "We start by getting a list of supported models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7643bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = json.loads(client.models.list().json())[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e14e3d",
   "metadata": {},
   "source": [
    "In total we have 32 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96f6b9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e11616",
   "metadata": {},
   "source": [
    "Along with some information about each model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "102a537d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'dall-e-3',\n",
       "  'created': 1698785189,\n",
       "  'object': 'model',\n",
       "  'owned_by': 'system'},\n",
       " {'id': 'whisper-1',\n",
       "  'created': 1677532384,\n",
       "  'object': 'model',\n",
       "  'owned_by': 'openai-internal'},\n",
       " {'id': 'davinci-002',\n",
       "  'created': 1692634301,\n",
       "  'object': 'model',\n",
       "  'owned_by': 'system'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f448b4",
   "metadata": {},
   "source": [
    "But let's just get a list of model names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42962461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage-002\n",
      "dall-e-2\n",
      "dall-e-3\n",
      "davinci-002\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo-0301\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-16k-0613\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4\n",
      "gpt-4-0125-preview\n",
      "gpt-4-0613\n",
      "gpt-4-1106-preview\n",
      "gpt-4-1106-vision-preview\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4-turbo-preview\n",
      "gpt-4-vision-preview\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-test-shared\n",
      "text-embedding-3-large\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n",
      "tts-1\n",
      "tts-1-1106\n",
      "tts-1-hd\n",
      "tts-1-hd-1106\n",
      "whisper-1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(sorted([model[\"id\"] for model in model_list])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770f275",
   "metadata": {},
   "source": [
    "## Basic Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce70f1",
   "metadata": {},
   "source": [
    "The recommended model for exploration is `gpt-3.5-turbo`, so we'll stick with it for now. The basic setup is relatively straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adafc6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.05 ms, sys: 1.79 ms, total: 7.84 ms\n",
      "Wall time: 873 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What was Superman's weakness?\"\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3d11a",
   "metadata": {},
   "source": [
    "Which produces a response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44cbe6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.ChatCompletion"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b82bc",
   "metadata": {},
   "source": [
    "Which we can treat as a named tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a35e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Superman's weakness is kryptonite, a mineral from his home planet of Krypton that weakens and can even kill him.\", role='assistant', function_call=None, tool_calls=None))]\n"
     ]
    }
   ],
   "source": [
    "pprint(response.choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0fb2d",
   "metadata": {},
   "source": [
    "The model answer can be found in the \"message\" dictionary inside the \"choices\" list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "832a11d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Superman's weakness is kryptonite, a mineral from his home planet of Krypton that weakens and can even kill him.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f96500",
   "metadata": {},
   "source": [
    "To request multiple answers, we must include the `n` parameter with the number of answers we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "934d4574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.78 ms, sys: 1.87 ms, total: 7.64 ms\n",
      "Wall time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What are the different kinds of Kryptonite?\"},\n",
    "    ],\n",
    "    n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c985ec",
   "metadata": {},
   "source": [
    "And we can access each of the answers individually int he choices list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bcad922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Assistant\n",
      "==========\n",
      "There are several different types of Kryptonite, each with unique effects on Superman depending on its color:\n",
      "\n",
      "1. Green Kryptonite: This is the most common form of Kryptonite and is deadly to Superman. Exposure to green Kryptonite weakens him, making him lose his powers and eventually leading to his death if he is not removed from its presence.\n",
      "\n",
      "2. Red Kryptonite: Red Kryptonite has unpredictable effects on Superman, with each exposure causing different temporary changes or distortions in his abilities or behavior.\n",
      "\n",
      "3. Blue Kryptonite: Blue Kryptonite is specifically harmful to Bizarro, a flawed clone of Superman. It has no effect on the real Superman.\n",
      "\n",
      "4. Gold Kryptonite: Gold Kryptonite permanently removes Superman's powers and abilities, essentially turning him into a normal human being.\n",
      "\n",
      "5. White Kryptonite: White Kryptonite is lethal to all plant life, causing it to wither and die. It has no effect on Superman.\n",
      "\n",
      "6. Black Kryptonite: Black Kryptonite has the ability to split a person into two separate beings, each representing different aspects of their personality.\n",
      "==========\n",
      "\n",
      "==========\n",
      "Assistant\n",
      "==========\n",
      "- Green Kryptonite: The most common form of Kryptonite, green Kryptonite is deadly to Kryptonians and weakens Superman's abilities.\n",
      "- Red Kryptonite: Red Kryptonite has unpredictable effects on Kryptonians, causing temporary changes in Superman's behavior, powers, or physical appearance.\n",
      "- Blue Kryptonite: Blue Kryptonite affects Bizarro, a flawed clone of Superman, in the same way that green Kryptonite affects Superman.\n",
      "- Gold Kryptonite: Gold Kryptonite permanently removes a Kryptonian's powers.\n",
      "- White Kryptonite: White Kryptonite is deadly to plant life, causing plants to wither and die.\n",
      "- Black Kryptonite: Black Kryptonite has the ability to split a Kryptonian into two separate beings, one good and one evil.\n",
      "- Anti-Kryptonite: Anti-Kryptonite has the opposite effect of green Kryptonite, enhancing the powers of Kryptonians rather than weakening them.\n",
      "==========\n",
      "\n",
      "==========\n",
      "Assistant\n",
      "==========\n",
      "1. Green Kryptonite - the most common form that weakens Superman and other Kryptonians, causing them pain and potential death if exposed for too long.\n",
      "2. Red Kryptonite - alters the personality and behavior of Kryptonians, causing them to act out of character.\n",
      "3. Blue Kryptonite - specifically affects Bizarro, a flawed clone of Superman, weakening him instead of Superman.\n",
      "4. Gold Kryptonite - permanently strips Kryptonians of their powers.\n",
      "5. White Kryptonite - lethal to plant life and can harm plant-based beings.\n",
      "6. Black Kryptonite - can split a Kryptonian into two separate beings, one good and one evil, when exposed.\n",
      "7. Anti-Kryptonite - acts as a reverse Kryptonite, providing power and strength to Kryptonians instead of weakening them.\n",
      "==========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in response.choices:\n",
    "    print(\"==========\")\n",
    "    print(output.message.role.title()) \n",
    "    print(\"==========\")\n",
    "    print(output.message.content)\n",
    "    print(\"==========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "279d48c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=607, prompt_tokens=17, total_tokens=624)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18509d4",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ddb64d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.68 ms, sys: 1.4 ms, total: 7.08 ms\n",
      "Wall time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a short story\"},\n",
    "    ],\n",
    "    temperature=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22fd09b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a school that held a talent show every year. This year, Lily had decided to participate. Despite being a bit nervous, she chose to sing because it was something that had always brought her joy.\n",
      "\n",
      "However, as the day of the talent show approached, Lily's nerves got the best of her and she began to doubt herself. On the day of the talent show, she felt sick to her stomach and barely ate breakfast.\n",
      "\n",
      "But when it was her turn to perform, something incredible happened. As soon as she started singing, all her doubts and fears disappeared. She put her heart and soul into the performance and sang with such emotion that the entire audience was captivated.\n",
      "\n",
      "When she finished, there was a moment of silence before the whole room erupted in applause. Lily couldn't believe it. The headteacher even stood up and declared her performance the best of the night.\n",
      "\n",
      "From that moment on, Lily gained confidence in her talent and realized that as long as she stayed true to herself, she could achieve anything she set her mind to. And so, the talent show became a turning point in her life, making her more self-assured and ready to take on any challenge that came her way.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86beb86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.19 ms, sys: 2.57 ms, total: 10.8 ms\n",
      "Wall time: 3.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a short story\"},\n",
    "    ],\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1855d3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village nestled in the mountains, there lived a young girl named Lily. She was known for her kindness and generosity, always willing to help those in need.\n",
      "\n",
      "One day, a terrible storm hit the village, causing widespread damage and leaving many families homeless. Lily knew she had to do something to help. She gathered her friends and together they started a fundraiser to rebuild the homes that were destroyed.\n",
      "\n",
      "Through hard work and determination, they were able to raise enough money to not only rebuild the homes, but also provide food and clothing for those affected by the storm. The village was filled with gratitude for Lily and her friends' selfless actions.\n",
      "\n",
      "From that day on, Lily became known as the hero of the village, a shining example of how one person's kindness and compassion can make a difference in the lives of others. And she continued to spread love and positivity wherever she went, inspiring those around her to do the same.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6e2bc",
   "metadata": {},
   "source": [
    "# Function Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ec02e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(messages, functions):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        # Define the functions the model is allowed to use\n",
    "        functions=functions\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85bfb94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_conversation(messages):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "    \n",
    "    for message in messages:\n",
    "        print(message)\n",
    "        if message[\"role\"] == \"system\":\n",
    "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message['role']]))\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message['role']]))\n",
    "        elif message[\"role\"] == \"assistant\" and message['function_call']:\n",
    "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message['role']]))\n",
    "        elif message[\"role\"] == \"assistant\" and not message['function_call']:\n",
    "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message['role']]))\n",
    "        elif message[\"role\"] == \"function\":\n",
    "            print(colored(f\"function ({message.name}): {message.content}\\n\", role_to_color[message.role]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44263a09",
   "metadata": {},
   "source": [
    "Let's create some function specifications to interface with a hypothetical weather API. We'll pass these function specification to the Chat Completions API in order to generate function arguments that adhere to the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d50ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\", \"format\"],\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe13f7",
   "metadata": {},
   "source": [
    "If we prompt the model about the current weather, it will respond with some clarifying questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dea82333",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "\n",
    "messages.append(\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n",
    "    })\n",
    "\n",
    "messages.append(\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"What's the weather like today\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa5dfb9",
   "metadata": {},
   "source": [
    "Once we provide the missing information, it will generate the appropriate function arguments for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82527ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \"I'm in New York, NY.\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82f112a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = chat(messages, functions=functions)\n",
    "assistant_message = chat_response.choices[0].message\n",
    "messages.append({\n",
    " \"role\":  assistant_message.role,\n",
    " \"content\":  assistant_message.content,\n",
    " \"function_call\":  assistant_message.function_call,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85060c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}\n",
      "\u001b[31msystem: Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\n",
      "\u001b[0m\n",
      "{'role': 'user', 'content': \"What's the weather like today\"}\n",
      "\u001b[32muser: What's the weather like today\n",
      "\u001b[0m\n",
      "{'role': 'user', 'content': \"I'm in New York, NY.\"}\n",
      "\u001b[32muser: I'm in New York, NY.\n",
      "\u001b[0m\n",
      "{'role': 'assistant', 'content': None, 'function_call': FunctionCall(arguments='{\"location\":\"New York, NY\",\"format\":\"celsius\"}', name='get_current_weather')}\n",
      "\u001b[34massistant: FunctionCall(arguments='{\"location\":\"New York, NY\",\"format\":\"celsius\"}', name='get_current_weather')\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8a12a1",
   "metadata": {},
   "source": [
    "## Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca0dde",
   "metadata": {},
   "source": [
    "We can also provide several examples of mappings between input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80a730c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This last-minute change means we can't spend a lot of time on the client's project.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
    "        {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "        {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d83c1cb",
   "metadata": {},
   "source": [
    "# Formatted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc5d104c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.55 ms, sys: 1.95 ms, total: 11.5 ms\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "userInput = \"blueberry pancakes\"\n",
    "\n",
    "prompt = \"\"\"return a recipe for %s.\n",
    "        Provide your response as a JSON object with the following schema:\n",
    "        {\"dish\": \"%s\", \"ingredients\": [\"\", \"\", ...],\n",
    "        \"instructions\": [\"\", \"\", ... ]}\"\"\" % (userInput, userInput)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "          model = \"gpt-3.5-turbo\",\n",
    "          messages = [\n",
    "            { \"role\": \"system\", \"content\": \"You are a helpful recipe assistant.\"},\n",
    "            { \"role\": \"user\",   \"content\": prompt }\n",
    "          ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7e701cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9733d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = json.loads(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d03723fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 cup all-purpose flour',\n",
       " '1 tablespoon sugar',\n",
       " '1 teaspoon baking powder',\n",
       " '1/2 teaspoon baking soda',\n",
       " '1/4 teaspoon salt',\n",
       " '1 cup buttermilk',\n",
       " '1 large egg',\n",
       " '2 tablespoons melted butter',\n",
       " '1 cup fresh blueberries',\n",
       " 'Butter or oil for cooking']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"ingredients\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db048b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In a large bowl, whisk together the flour, sugar, baking powder, baking soda, and salt.',\n",
       " 'In a separate bowl, whisk together the buttermilk, egg, and melted butter.',\n",
       " 'Pour the wet ingredients into the dry ingredients and stir until just combined. The batter may be slightly lumpy.',\n",
       " 'Gently fold in the fresh blueberries.',\n",
       " 'Heat a non-stick skillet or griddle over medium heat and add a little butter or oil.',\n",
       " 'Pour 1/4 cup of batter for each pancake onto the skillet. Cook until bubbles form on the surface of the pancakes and the edges look set, about 2-3 minutes.',\n",
       " 'Flip the pancakes and cook for another 1-2 minutes, or until golden brown.',\n",
       " 'Repeat with the remaining batter, adding more butter or oil to the skillet as needed.',\n",
       " 'Serve the blueberry pancakes warm with maple syrup or your favorite pancake toppings.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"instructions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811b139",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a0e7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[{\"role\": \"system\", \"content\": \"You're a professional English-Italian translator.\"}, \n",
    "              {\"role\": \"user\", \"content\": \"Translate 'Be the change that you wish to see in the world.' into Italian\"}],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca268762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Sii il cambiamento che desideri vedere nel mondo.\"'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc3b0cc",
   "metadata": {},
   "source": [
    "# Process unstructured information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cfe4a",
   "metadata": {},
   "source": [
    "Inspired by https://platform.openai.com/examples/default-parse-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0d87024",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"There are many fruits that were found on the recently discovered planet Goocrux. \n",
    "There are neoskizzles that grow there, which are purple and taste like candy. There are also \n",
    "loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits \n",
    "are a bright green color and are more savory than sweet. There are also plenty of loopnovas which \n",
    "are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which \n",
    "have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0354c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": \"You will be provided with unstructured data, and your task is to parse it into CSV format.\"}, \n",
    "        {\"role\": \"user\", \n",
    "         \"content\": prompt}],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7529da42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruit,Color,Flavor\n",
      "neoskizzles,Purple,Candy\n",
      "loheckles,Grayish blue,Tart\n",
      "pounits,Bright green,Savory\n",
      "loopnovas,Neon pink,Cotton candy\n",
      "glowls,Pale orange,Sour and bitter\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0daaad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo',\n",
    "    messages=[{\"role\": \"system\", \"content\": \"\"\"\n",
    "            Read this paragraph \n",
    "            \n",
    "            `%s` \n",
    "            \n",
    "            and use it to answer some questions.\"\"\" % prompt}, \n",
    "              {\"role\": \"user\", \"content\": \"What are pounits?\"}],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94e1155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pounits are bright green fruits that are more savory than sweet.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec8a68a",
   "metadata": {},
   "source": [
    "<center>\n",
    "     <img src=\"https://raw.githubusercontent.com/DataForScience/Networks/master/data/D4Sci_logo_full.png\" alt=\"Data For Science, Inc\" align=\"center\" border=\"0\" width=300px> \n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
